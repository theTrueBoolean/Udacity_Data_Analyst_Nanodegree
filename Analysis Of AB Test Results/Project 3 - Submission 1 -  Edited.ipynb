{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Test Results Analysis\n",
    "\n",
    "### Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "In this project, I'll be analysing the results from an A/B test for a company's e-commerce website, working to understand those results, and proffer helpful suggestions. The company has developed a new web page in order to try and increase the number of users who \"convert\" (i.e. the number of users who decide to pay for the company's product). The goal here is to help the company decide whether to implement the new page, keep the old page, or perhaps run the experiment longer before making a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info on datasets\n",
    "1. **`ab_data.csv` dataset**\n",
    "\n",
    "This dataset contains information about the `200K+` users involved in the A/B test.\n",
    "* `user_id` - unique identifier for each user\n",
    "* `timestamp` - associated date and time for each visit to the website by a given user\n",
    "* `group` - the category a user was grouped into pre-A/B test (`control` or `treatment` groups)\n",
    "* `landing_page` - the page that was displayed to a user when they visited the company website (`new_page` or `old_page`)\n",
    "* `converted` - whether a user converted or not (`0` or `1`)\n",
    "**NB:** Users in the control group ought to be displayed the old page, while those in the treatment group ought to see the new page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='probability'></a>\n",
    "#### Part I - Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#I am setting a random seed to ensure reproducibility of results\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the `ab_data.csv` dataset and take a look at the top few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for inconvenient data types and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294478 non-null  int64 \n",
      " 1   timestamp     294478 non-null  object\n",
      " 2   group         294478 non-null  object\n",
      " 3   landing_page  294478 non-null  object\n",
      " 4   converted     294478 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `timestamp` column is an `object` type, a `datetime` tyope would be more appropriate.\n",
    "\n",
    "There are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'timestamp' to datetime for easier manipulation\n",
    "df.timestamp = pd.to_datetime(df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                  int64\n",
       "timestamp       datetime64[ns]\n",
       "group                   object\n",
       "landing_page            object\n",
       "converted                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that worked\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some preliminary stats about the results presented in the dataset.\n",
    "\n",
    "First, let's see the number of visits that were made to the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website was visited 294478 times.\n"
     ]
    }
   ],
   "source": [
    "print('The website was visited', df.shape[0], 'times.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 290584 unique users.\n"
     ]
    }
   ],
   "source": [
    "print('There are', df.user_id.nunique(), 'unique users.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For how long was the test conducted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The A/B test was conducted for 23 days.\n"
     ]
    }
   ],
   "source": [
    "print('The A/B test was conducted for', len(df.timestamp.dt.floor('d').value_counts()), 'days.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now I'll check what proportion of users converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11965919355605512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.converted == 1].shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 12% of the visits to the website resulted in conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results\n",
    "\n",
    "Our A/B test is contigent on all members of the control group having viewed the old page, and those in the treatment group having viewed the new page. Hence, we must ensure that this is the case in our dataset, and get rid of any entries that do not satisfy this criterion for whatever reason (could be due to the mismatching of group members to their corresponding pages, or simply an error occurred during data gathering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Firstly, I'll check the number of times the `new_page` and `treatment` don't line up. Hopefully it isn't the majority of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[((df.group=='treatment') & (df.landing_page!='new_page')) | \n",
    "   ((df.group!='treatment') & (df.landing_page=='new_page'))].shape[0]\n",
    "# OR\n",
    "# df[((df['group'] == 'treatment') ^ (df['landing_page'] == 'new_page'))].shape[0]\n",
    "# OR\n",
    "# df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very many compared to the total entries in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the rows where `treatment` is not aligned with `new_page` or `control` is not aligned with `old_page`, we cannot be sure if this row truly received the new or old page. So we drop all the rows that don't meet the specifications.\n",
    "I assign the result to a new dataframe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of exclusive OR (XOR) to find disalignment between page and corresponding group\n",
    "df2 = df.drop(df[((df['group'] == 'treatment') ^ (df['landing_page'] == 'new_page'))].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should return 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to ensure that the new dataset does not contain repeated entries for a given user, in order to avoid misleading results.\n",
    "\n",
    "Compare number of unique ids to total number of rows in df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290585,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.user_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be one duplicate row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                  timestamp      group landing_page  converted\n",
       "1899   773192 2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192 2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.user_id.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first duplicate with index 1899\n",
    "df2.drop([1899], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some essential probabilities\n",
    "\n",
    "Next, I am going to find the probability of an individual converting regardless of the page they received?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.converted.sum()/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is pretty close to the one we got from the previous unfiltered dataset: approximately `12%`.\n",
    "\n",
    "Now I want to find the conversion rate or probability of conversion, given that an individual was in the `control` group, and also given that an individual was in the `treatment` grouup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control group conversion rate\n",
    "ctrl = df2[df2.group=='control']\n",
    "p_ctrl = ctrl.converted.sum()/ctrl.shape[0]\n",
    "p_ctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treatment group conversion rate\n",
    "treat = df2[df2.group=='treatment']\n",
    "p_treat = treat.converted.sum()/treat.shape[0]\n",
    "p_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to be getting quite similar values in all cases: roughly `12%`.\n",
    "\n",
    "So, let's see the probability that an individual actually received the new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df2.landing_page=='new_page').sum()/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would expect, an individual receives the new page half of the time, which might be why the conversion rates for both pages are quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the calculations of the values for `p_treat`(conversion rate for treatment group) and `p_ctrl`((conversion rate for control group)) above, we see that the difference between the conversion rate for individuals who were shown the old page and those shown the new page (i.e. `p_ctrl` - `p_treat`) is approximately `0.00` (since both values are about the same). Hence, there doesn't seem to be sufficient evidence to suggest that the new treatment page leads to more conversions. Instead, the conversion rate for the new page appears to be slightly smaller than that of the old page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### A/B Test\n",
    "\n",
    "Now for the A/B test analysis!\n",
    "\n",
    "First, I'll set up my null and alternative hypotheses.\n",
    "\n",
    "**Null:** For the null, I want to assume that the old page performs better (i.e. it has a higher conversion rate) than the new page.\n",
    "\n",
    "**Alternative:** There is enough evidence to show that the old page performs worse, in which case I'll suggest the company opts for the new page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_{o}$:&emsp; $p_{new}$ - $p_{old}$ $\\leq$ 0\n",
    "\n",
    "$H_{1}$:&emsp; $p_{new}$ - $p_{old}$ $\\gt$ 0\n",
    "\n",
    "_where $p_{old}$ is the conversion rate for the old page, while $p_{new}$ is the conversion rate for the new page_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the hypothesis definitions above, the **metric** I am measuring is the **difference between the conversion rates for the old and new pages.**\n",
    "\n",
    "I assume, under the null hypothesis, that $p_{new}$ and $p_{old}$ both have conversion rates equal to the overall conversion rate in the dataset (regardless of the page a user was shown). By consequence, $p_{new}$ and $p_{old}$ are equal, and their difference is `0`.\n",
    "\n",
    "Most probably, the data in the original dataset was gathered such that it is sufficiently random, and represents the population fairly well. Hence, I'll use the sample-sizes for both treatment and control groups in the sampling distribution simulation.\n",
    "\n",
    "Now, I'm going to simulate a sampling distribution for both the old and new pages, having conversion rates of $p_{old}$ and $p_{new}$, and sample sizes of $n_{old}$ and $n_{new}$ respectively. And then I'll take the difference between the conversion rates of these two distribution, to ensure they correspond to the null hypothesis (i.e. the difference is close to `0`), in which case we can be more or less confident that the chosen sample sizes are adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_old: 0.11959708724499628\n",
      "p_new: 0.11959708724499628\n",
      "n_old: 145274\n",
      "n_new: 145310\n"
     ]
    }
   ],
   "source": [
    "# set conversion rates and sample sizes for both groups under the null\n",
    "p_old = df2.converted.mean()\n",
    "p_new = df2.converted.mean()\n",
    "n_old = df2[df2.landing_page == 'old_page'].shape[0]\n",
    "n_new = df2[df2.landing_page == 'new_page'].shape[0]\n",
    "print(f\"p_old: {p_old}\\np_new: {p_new}\\nn_old: {n_old}\\nn_new: {n_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0005526846951238451"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulation of the two binomial distributions and the difference in their conversion rates\n",
    "old_page_converted = np.random.binomial(1, p=p_old, size=n_old)\n",
    "new_page_converted = np.random.binomial(1, p=p_new, size=n_new)\n",
    "diff = new_page_converted.mean() - old_page_converted.mean()\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the difference value above (approximately `0.00`).\n",
    "So it seems the sample sizes would do the trick.\n",
    "\n",
    "Next, I'll be simulating a sampling distribution for the difference in conversion rates between the old and new pages (just as was done above), but this time over 10,000 iterations, to be certain the above results aren't merely due to chance. I'll store this distribution in a numpy array. All previous parameters hold constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the difference between the conversion rate for new and old pages\n",
    "# make use of binomial distribution since that fits our scenario\n",
    "new_page_converted = np.random.binomial(n_new, p_new, 10000) #returns no. of successes from n_new trials,performed 10000 times\n",
    "old_page_converted = np.random.binomial(n_old, p_old, 10000) #returns no. of successes from n_old trials,performed 10000 times\n",
    "#NB: we cannot use new_page_converted.mean() as above since our simulation returns the no. of successes and not 0s and 1s\n",
    "p_diffs = new_page_converted/n_new - old_page_converted/n_old\n",
    "p_diffs = np.array(p_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a histogram of the sampling distribution above (`p_diffs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_diffs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the plot shows that the sampling distribution of the differences `p_diffs`, is normally distributed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### P-value\n",
    "To get the p-value, I'll find the proportion of difference values in `p_diffs` that are equal to, or more extreme than (i.e. in favour of the alternative) the actual difference observed from the original data. This would mean differences which are greater than (in accordance with the null hypothesis) the observed difference.\n",
    "\n",
    "**NB:** The observed difference is the conversion rate for both the old and new pages as observed from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9058"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get observed difference first, then determine the more extreme values in favour of the alternative\n",
    "obs_diff = (df2[df2.group=='treatment'].converted.mean()) - (df2[df2.group=='control'].converted.mean())\n",
    "p_val = (p_diffs > obs_diff).mean()\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I've computed the probability of obtaining our observed statistic, `obs_diff`, or a more extreme value (in favour of the alternative hypothesis) based on the premise that the null hypothesis is true. This probability is the **p-value** for the sampling distribution of differences.\n",
    "\n",
    "Since this value is greater than the our designated significance level of **alpha=0.05**, then we may conclude that, based on the data, we do not have sufficient evidence in favour of the alternative hypothesis, hence, the result is not statistically significant and we do not reject the null hypothesis. In other words, the difference between the conversion rate for the old page and that of the new page is less than or equal to `0`, or otherwise put: **the conversion rate for the old page is either greater than or equal to that of the new page.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to also make use of a built-in to ensure I achieve similar results.\n",
    "\n",
    "I'll be making use of the `statsmodels` library in Python, and the same parameters as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = df2[df2.landing_page=='old_page'].converted.sum()\n",
    "convert_new = df2[df2.landing_page=='new_page'].converted.sum()\n",
    "n_old = df2[df2.landing_page=='old_page'].shape[0]\n",
    "n_new = df2[df2.landing_page=='new_page'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to compute the test statistic (z-score) and the p-value, I'll use the `stats.proportions_ztest` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score: 1.3109241984234394\n",
      "p-value: 0.9050583127590245\n"
     ]
    }
   ],
   "source": [
    "test_stat, p_value = sm.stats.proportions_ztest(np.array([convert_old, convert_new]), np.array([n_old, n_new]), alternative='smaller')\n",
    "print(f\"z-score: {test_stat}\\np-value: {p_value}\") # test_stat is the z-score for our p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score and p-value obtained above imply that there is no statistical evidence to support the change in the webpage, since there is no increase in the conversion rate for the new page (i.e. both pages have similar conversion rates, or old is greater than new) according to the data.\n",
    "\n",
    "Indeed, these results are in agreement with the findings from the simulated distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "#### A regression approach\n",
    "\n",
    "In this final part, I will be using regression in order to, hopefully, achieve similar results as in the previous methods.\n",
    "\n",
    "A logistic regression would be the appropriate kind of regression in this case, as it involves the determination of which category a given dataset falls into (i.e. conversion or no conversion).\n",
    "\n",
    "The goal here is to use `statsmodels` to fit a logistic regression model to see if there is a significant difference in conversion as a consequence of which page a customer receives.\n",
    "\n",
    "To use `statsmodels`, I'll first need to create a column for the intercept, and create a dummy variable column for which page each user received.\n",
    "\n",
    "In essence, I'll add an `intercept` column with all `1`'s, as well as an `ab_page` column, which is 1 when an individual receives the new page (**treatment**) and 0 if they recieved the the old page(**control**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                  timestamp      group landing_page  converted  \\\n",
       "0   851104 2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228 2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590 2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541 2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975 2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['intercept'] = 1\n",
    "df2['ab_page'] = pd.get_dummies(df2.group)['treatment']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model, and fit the model using the two columns earlier created to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "log_mod_1 = sm.Logit(df2.converted, df2[['intercept', 'ab_page']])\n",
    "result_1 = log_mod_1.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 16 Jul 2020</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:43:47</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Thu, 16 Jul 2020   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        17:43:47   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the regression summary above, the p-value associated with the **ab_page** variable is `0.190`. This value differs from the p-value obtained using A/B testing, which is as a result of the difference in how the hypotheses were set up.\n",
    "\n",
    "In the A/B testing scenario, I used a `one-tailed test`: we were interested in an observed change in just one direction away from our metric i.e.\n",
    "\n",
    "$H_{o}$: $p_{new}$ - $p_{old}$ $\\leq$ 0,\n",
    "\n",
    "$H_{1}$: $p_{new}$ - $p_{old}$ $\\gt$ 0;\n",
    "\n",
    "whereas, in the regression case above, I make use of a `two-tailed test`: we are concerned with a change in either direction of the metric being measured i.e.\n",
    "\n",
    "$H_{o}$: $p_{new}$ - $p_{old}$ = 0,\n",
    "\n",
    "$H_{1}$: $p_{new}$ - $p_{old}$ $\\neq$ 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am considering adding other variables that might influence whether or not an individual converts. Would this be a good idea?\n",
    "\n",
    "From the summary table above, the independent variable, `ab_page`, has a p-value of `0.190`, which implies that the variable is not statistically significant (since p>$\\alpha$). This means that there is no observable change in the conversion rate that may be as a consequence of a change in the webpage that a user is shown. As a result, it would make sense to consider other factors that might influence a change in the conversion rate. Also, from the `McFadden pseudo-R squared` value in the summary, it is quite clear that the `ab_page` feature accounts for a very meager amount(practically zero) of the variability in the dependent variable (i.e. the conversion rate), hence, the need to explore other variables for better explainability.\n",
    "\n",
    "The effect of inserting an additional feature into the model would depend partly (among other factors) on whether the variable is correlated with any of the already existing features, and/or with the response variable. In a case where there is a correlation, the added variable would definitely have a negative effect on the overall quality of the model. In the alternate scenario in which absolutely no correlation is present (which never really happens in practice: we generally decide on what level of collinearity is tolerable), then the additional variable contributes positively to the explainability of the model.\n",
    "\n",
    "So, in the quest for better explainability, I'll be adding a new feature: **the country in which a user lives**. I'll read in the **countries.csv** dataset and merge it to the original dataset on the `user_id`. Again I'll create dummy variables for the `country` column. Let's find out if this added variable is a better predictor of conversion rate.So, in the quest for better explainability, I'll be adding a new feature: **the country in which a user lives**. I'll read in the **countries.csv** dataset and merge it to the original dataset on the `user_id`. Again I'll create dummy variables for the `country` column. Let's find out if this added variable is a better predictor of conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                  timestamp      group landing_page  converted  \\\n",
       "user_id                                                                         \n",
       "834778       UK 2017-01-14 23:08:43.304998    control     old_page          0   \n",
       "928468       US 2017-01-23 14:44:16.387854  treatment     new_page          0   \n",
       "822059       UK 2017-01-16 14:04:14.719771  treatment     new_page          1   \n",
       "711597       UK 2017-01-22 03:14:24.763511    control     old_page          0   \n",
       "710616       UK 2017-01-16 13:14:44.000513  treatment     new_page          0   \n",
       "\n",
       "         intercept  ab_page  \n",
       "user_id                      \n",
       "834778           1        0  \n",
       "928468           1        1  \n",
       "822059           1        1  \n",
       "711597           1        0  \n",
       "710616           1        1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_df = pd.read_csv('./countries.csv')\n",
    "df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UK', 'US', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique entries in 'country' column\n",
    "df_new.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 03 Aug 2020</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>19:46:43</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -2.0300</td> <td>    0.027</td> <td>  -76.249</td> <td> 0.000</td> <td>   -2.082</td> <td>   -1.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0506</td> <td>    0.028</td> <td>    1.784</td> <td> 0.074</td> <td>   -0.005</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>    0.0408</td> <td>    0.027</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.012</td> <td>    0.093</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Mon, 03 Aug 2020   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        19:46:43   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -2.0300      0.027    -76.249      0.000      -2.082      -1.978\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "UK             0.0506      0.028      1.784      0.074      -0.005       0.106\n",
       "US             0.0408      0.027      1.516      0.130      -0.012       0.093\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the necessary dummy variables.\n",
    "### I'll create dummies for UK and US alone, leaving CA as the baseline.\n",
    "df_new[['UK', 'US']] = pd.get_dummies(df_new.country)[['UK','US']]\n",
    "\n",
    "log_mod_2 = sm.Logit(df_new.converted, df_new[['intercept', 'ab_page', 'UK', 'US']])\n",
    "result_2 = log_mod_2.fit()\n",
    "# get model summary\n",
    "result_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary above, we see that the p-values for both UK and US (`0.074` and `0.130` respectively) are less than the chosen $\\alpha$-value of 0.05 and are thus not significant covariates of the model. Also, the pseudo R-squared value (though not a good estimate of explainability) indicates that the  added country variables did not substantially contribute to the explainability of the model. In sum, the `country` variable does not seem to have any impact on conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have now looked at the individual factors of **country** and **page** on conversion. I would now like to look at an interaction between page and country to see if that would have a significant effect on conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the additional columns for the interactions\n",
    "df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page']\n",
    "df_new['US_ab_page'] = df_new['US'] * df_new['ab_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>UK_ab_page</th>\n",
       "      <th>US_ab_page</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                  timestamp      group landing_page  converted  \\\n",
       "user_id                                                                         \n",
       "834778       UK 2017-01-14 23:08:43.304998    control     old_page          0   \n",
       "928468       US 2017-01-23 14:44:16.387854  treatment     new_page          0   \n",
       "822059       UK 2017-01-16 14:04:14.719771  treatment     new_page          1   \n",
       "711597       UK 2017-01-22 03:14:24.763511    control     old_page          0   \n",
       "710616       UK 2017-01-16 13:14:44.000513  treatment     new_page          0   \n",
       "\n",
       "         intercept  ab_page  UK  US  UK_ab_page  US_ab_page  \n",
       "user_id                                                      \n",
       "834778           1        0   1   0           0           0  \n",
       "928468           1        1   0   1           0           1  \n",
       "822059           1        1   1   0           1           0  \n",
       "711597           1        0   1   0           0           0  \n",
       "710616           1        1   1   0           1           0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 03 Aug 2020</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>19:57:50</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>   -2.0040</td> <td>    0.036</td> <td>  -55.008</td> <td> 0.000</td> <td>   -2.075</td> <td>   -1.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>    <td>   -0.0674</td> <td>    0.052</td> <td>   -1.297</td> <td> 0.195</td> <td>   -0.169</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>         <td>    0.0118</td> <td>    0.040</td> <td>    0.296</td> <td> 0.767</td> <td>   -0.066</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>         <td>    0.0175</td> <td>    0.038</td> <td>    0.465</td> <td> 0.642</td> <td>   -0.056</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK_ab_page</th> <td>    0.0783</td> <td>    0.057</td> <td>    1.378</td> <td> 0.168</td> <td>   -0.033</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US_ab_page</th> <td>    0.0469</td> <td>    0.054</td> <td>    0.872</td> <td> 0.383</td> <td>   -0.059</td> <td>    0.152</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Mon, 03 Aug 2020   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        19:57:50   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1920\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -2.0040      0.036    -55.008      0.000      -2.075      -1.933\n",
       "ab_page       -0.0674      0.052     -1.297      0.195      -0.169       0.034\n",
       "UK             0.0118      0.040      0.296      0.767      -0.066       0.090\n",
       "US             0.0175      0.038      0.465      0.642      -0.056       0.091\n",
       "UK_ab_page     0.0783      0.057      1.378      0.168      -0.033       0.190\n",
       "US_ab_page     0.0469      0.054      0.872      0.383      -0.059       0.152\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fit the Linear Model And Obtain the Results\n",
    "log_mod_3 = sm.Logit(df_new.converted, df_new[['intercept','ab_page', 'UK', 'US', 'UK_ab_page', 'US_ab_page']])\n",
    "result_3 = log_mod_3.fit()\n",
    "result_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conclusions\n",
    "* From the summary above, the interactions between the page shown to a user and the country that user comes from does not influence the conversion of the user. This is because the p-values for the interaction variables with respect to the dependent variable are all $\\gt \\alpha$ (0.05), meaning they are not statistically significant.\n",
    "* Practically speaking, the country from which a user visits a webpage is a large categorisation, and, hence, isn't likely to have much predictive power on its own, except perhaps more related information is given (like government policy vis-a-vis the web, internet censorship, etc).\n",
    "* In sum, statistically speaking, the webpage, country, and the interaction between both of these variables, do not appear to have any impact on the conversion of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation\n",
    "\n",
    "Based on all the evidence from the A/B test and logistic regression, there is no evidence that the new page would improve the conversion rate of the e-commerce company. In that light, I would recommend that the company choose to stick to the old page for the mean time, while researching on website features that would positively influence the chosen metric, and then another A/B test could be run to measure that influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### References\n",
    "* https://www.intechopen.com/books/bayesian-inference/bayesian-hypothesis-testing-an-alternative-to-null-hypothesis-significance-testing-nhst-in-psycholog\n",
    "* http://www.real-statistics.com/hypothesis-testing/null-hypothesis/\n",
    "* https://stats.stackexchange.com/questions/52067/does-adding-more-variables-into-a-multivariable-regression-change-coefficients-o\n",
    "* https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/\n",
    "* https://www.pluralsight.com/guides/interpreting-data-using-statistical-models-python\n",
    "* https://www.displayr.com/how-to-interpret-logistic-regression-coefficients/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
